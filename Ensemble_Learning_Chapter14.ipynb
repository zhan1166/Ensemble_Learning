{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习案例一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**背景介绍**\n",
    "\n",
    "此案例是一个数据挖掘类型的比赛——幸福感预测的baseline。比赛的数据使用的是官方的《中国综合社会调查（CGSS）》文件中的调查结果中的数据，其共包含有139个维度的特征，包括个体变量（性别、年龄、地域、职业、健康、婚姻与政治面貌等等）、家庭变量（父母、配偶、子女、家庭资本等等）、社会态度（公平、信用、公共服务）等特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据信息**\n",
    "\n",
    "赛题要求使用以上 139 维的特征，使用 8000 余组数据进行对于个人幸福感的预测（预测值为1，2，3，4，5，其中1代表幸福感最低，5代表幸福感最高）。 因为考虑到变量个数较多，部分变量间关系复杂，数据分为完整版和精简版两类。可从精简版入手熟悉赛题后，使用完整版挖掘更多信息。在这里我直接使用了完整版的数据。赛题也给出了index文件中包含每个变量对应的问卷题目，以及变量取值的含义；survey文件中为原版问卷，作为补充以方便理解问题背景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**评价指标**\n",
    "\n",
    "最终的评价指标为均方误差MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import os\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error,mean_absolute_error, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.ensemble import ExtraTreesRegressor as etr\n",
    "from sklearn.linear_model import BayesianRidge as br\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import ElasticNet as en\n",
    "from sklearn.kernel_ridge import KernelRidge as kr\n",
    "from sklearn.model_selection import  KFold, StratifiedKFold,GroupKFold, RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') #消除warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", parse_dates=['survey_time'],encoding='latin-1') \n",
    "test = pd.read_csv(\"test.csv\", parse_dates=['survey_time'],encoding='latin-1') #latin-1向下兼容ASCII\n",
    "train = train[train[\"happiness\"]!=-8].reset_index(drop=True)\n",
    "train_data_copy = train.copy() #删去\"happiness\" 为-8的行\n",
    "target_col = \"happiness\" #目标列\n",
    "target = train_data_copy[target_col]\n",
    "del train_data_copy[target_col] #去除目标列\n",
    "\n",
    "data = pd.concat([train_data_copy,test],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7988.000000\n",
       "mean        3.867927\n",
       "std         0.818717\n",
       "min         1.000000\n",
       "25%         4.000000\n",
       "50%         4.000000\n",
       "75%         4.000000\n",
       "max         5.000000\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.happiness.describe() #数据的基本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make feature +5\n",
    "#csv中有复数值：-1、-2、-3、-8，将他们视为有问题的特征，但是不删去\n",
    "def getres1(row):\n",
    "    return len([x for x in row.values if type(x)==int and x<0])\n",
    "\n",
    "def getres2(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-8])\n",
    "\n",
    "def getres3(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-1])\n",
    "\n",
    "def getres4(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-2])\n",
    "\n",
    "def getres5(row):\n",
    "    return len([x for x in row.values if type(x)==int and x==-3])\n",
    "\n",
    "#检查数据\n",
    "data['neg1'] = data[data.columns].apply(lambda row:getres1(row),axis=1)\n",
    "data.loc[data['neg1']>20,'neg1'] = 20  #平滑处理\n",
    "\n",
    "data['neg2'] = data[data.columns].apply(lambda row:getres2(row),axis=1)\n",
    "data['neg3'] = data[data.columns].apply(lambda row:getres3(row),axis=1)\n",
    "data['neg4'] = data[data.columns].apply(lambda row:getres4(row),axis=1)\n",
    "data['neg5'] = data[data.columns].apply(lambda row:getres5(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey_type</th>\n",
       "      <th>province</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>survey_time</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>religion</th>\n",
       "      <th>...</th>\n",
       "      <th>public_service_5</th>\n",
       "      <th>public_service_6</th>\n",
       "      <th>public_service_7</th>\n",
       "      <th>public_service_8</th>\n",
       "      <th>public_service_9</th>\n",
       "      <th>neg1</th>\n",
       "      <th>neg2</th>\n",
       "      <th>neg3</th>\n",
       "      <th>neg4</th>\n",
       "      <th>neg5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>2015-08-04 14:18:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1959</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>2015-07-21 15:04:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>70</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>126</td>\n",
       "      <td>2015-07-21 13:24:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>2015-07-25 17:33:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>2015-08-10 09:50:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10951</th>\n",
       "      <td>10964</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>2015-07-19 20:01:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>10965</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>74</td>\n",
       "      <td>114</td>\n",
       "      <td>2015-08-08 13:09:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1977</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10953</th>\n",
       "      <td>10966</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>84</td>\n",
       "      <td>127</td>\n",
       "      <td>2015-07-22 09:29:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>10967</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>54</td>\n",
       "      <td>2015-07-20 16:06:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1950</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>10968</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>68</td>\n",
       "      <td>2015-09-03 09:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1941</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10956 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  survey_type  province  city  county         survey_time  gender  \\\n",
       "0          1            1        12    32      59 2015-08-04 14:18:00       1   \n",
       "1          2            2        18    52      85 2015-07-21 15:04:00       1   \n",
       "2          3            2        29    83     126 2015-07-21 13:24:00       2   \n",
       "3          4            2        10    28      51 2015-07-25 17:33:00       2   \n",
       "4          5            1         7    18      36 2015-08-10 09:50:00       2   \n",
       "...      ...          ...       ...   ...     ...                 ...     ...   \n",
       "10951  10964            1        27    77     117 2015-07-19 20:01:00       2   \n",
       "10952  10965            2        26    74     114 2015-08-08 13:09:00       2   \n",
       "10953  10966            2        29    84     127 2015-07-22 09:29:00       2   \n",
       "10954  10967            1        11    31      54 2015-07-20 16:06:00       1   \n",
       "10955  10968            1        13    37      68 2015-09-03 09:59:00       1   \n",
       "\n",
       "       birth  nationality  religion  ...  public_service_5  public_service_6  \\\n",
       "0       1959            1         1  ...              30.0                30   \n",
       "1       1992            1         1  ...              85.0                70   \n",
       "2       1967            1         0  ...              80.0                90   \n",
       "3       1943            1         1  ...              80.0                90   \n",
       "4       1994            1         1  ...              50.0                50   \n",
       "...      ...          ...       ...  ...               ...               ...   \n",
       "10951   1946            1         1  ...              50.0                60   \n",
       "10952   1977            1         1  ...              50.0                50   \n",
       "10953   1968            1         1  ...              60.0                60   \n",
       "10954   1950            1         1  ...              90.0                80   \n",
       "10955   1941            1         1  ...              90.0                90   \n",
       "\n",
       "      public_service_7  public_service_8  public_service_9  neg1  neg2  neg3  \\\n",
       "0                   50                50                50     5     3     0   \n",
       "1                   90                60                60     0     0     0   \n",
       "2                   90                90                75     3     1     0   \n",
       "3                   90                80                80     2     0     0   \n",
       "4                   50                50                50     2     1     1   \n",
       "...                ...               ...               ...   ...   ...   ...   \n",
       "10951               40                60                50     0     0     0   \n",
       "10952               50                50                50     2     0     0   \n",
       "10953               60                60                60     8     6     0   \n",
       "10954               80                80                80     4     1     1   \n",
       "10955               90                90                90    11     9     0   \n",
       "\n",
       "       neg4  neg5  \n",
       "0         2     0  \n",
       "1         0     0  \n",
       "2         2     0  \n",
       "3         2     0  \n",
       "4         0     0  \n",
       "...     ...   ...  \n",
       "10951     0     0  \n",
       "10952     2     0  \n",
       "10953     2     0  \n",
       "10954     2     0  \n",
       "10955     2     0  \n",
       "\n",
       "[10956 rows x 144 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填充缺失值 共25列 去掉4列 填充21列\n",
    "#以下的列都是缺省的，视情况填补\n",
    "data['work_status'] = data['work_status'].fillna(0)\n",
    "data['work_yr'] = data['work_yr'].fillna(0)\n",
    "data['work_manage'] = data['work_manage'].fillna(0)\n",
    "data['work_type'] = data['work_type'].fillna(0)\n",
    "\n",
    "data['edu_yr'] = data['edu_yr'].fillna(0)\n",
    "data['edu_status'] = data['edu_status'].fillna(0)\n",
    "\n",
    "data['s_work_type'] = data['s_work_type'].fillna(0)\n",
    "data['s_work_status'] = data['s_work_status'].fillna(0)\n",
    "data['s_political'] = data['s_political'].fillna(0)\n",
    "data['s_hukou'] = data['s_hukou'].fillna(0)\n",
    "data['s_income'] = data['s_income'].fillna(0)\n",
    "data['s_birth'] = data['s_birth'].fillna(0)\n",
    "data['s_edu'] = data['s_edu'].fillna(0)\n",
    "data['s_work_exper'] = data['s_work_exper'].fillna(0)\n",
    "\n",
    "data['minor_child'] = data['minor_child'].fillna(0)\n",
    "data['marital_now'] = data['marital_now'].fillna(0)\n",
    "data['marital_1st'] = data['marital_1st'].fillna(0)\n",
    "data['social_neighbor']=data['social_neighbor'].fillna(0)\n",
    "data['social_friend']=data['social_friend'].fillna(0)\n",
    "data['hukou_loc']=data['hukou_loc'].fillna(1) #最少为1，表示户口\n",
    "data['family_income']=data['family_income'].fillna(66365) #删除问题值后的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#144+1 =145\n",
    "#继续进行特殊的列进行数据处理\n",
    "#读happiness_index.xlsx\n",
    "data['survey_time'] = pd.to_datetime(data['survey_time'], format='%Y-%m-%d',errors='coerce')#防止时间格式不同的报错errors='coerce‘\n",
    "data['survey_time'] = data['survey_time'].dt.year #仅仅是year，方便计算年龄\n",
    "data['age'] = data['survey_time']-data['birth']\n",
    "# print(data['age'],data['survey_time'],data['birth'])\n",
    "#年龄分层 145+1=146\n",
    "bins = [0,17,26,34,50,63,100]\n",
    "data['age_bin'] = pd.cut(data['age'], bins, labels=[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey_type</th>\n",
       "      <th>province</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>survey_time</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>religion</th>\n",
       "      <th>...</th>\n",
       "      <th>public_service_7</th>\n",
       "      <th>public_service_8</th>\n",
       "      <th>public_service_9</th>\n",
       "      <th>neg1</th>\n",
       "      <th>neg2</th>\n",
       "      <th>neg3</th>\n",
       "      <th>neg4</th>\n",
       "      <th>neg5</th>\n",
       "      <th>age</th>\n",
       "      <th>age_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1959</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>126</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10951</th>\n",
       "      <td>10964</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>10965</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>74</td>\n",
       "      <td>114</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1977</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10953</th>\n",
       "      <td>10966</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>84</td>\n",
       "      <td>127</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>10967</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>54</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1950</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>10968</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>68</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1941</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10956 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  survey_type  province  city  county  survey_time  gender  birth  \\\n",
       "0          1            1        12    32      59         2015       1   1959   \n",
       "1          2            2        18    52      85         2015       1   1992   \n",
       "2          3            2        29    83     126         2015       2   1967   \n",
       "3          4            2        10    28      51         2015       2   1943   \n",
       "4          5            1         7    18      36         2015       2   1994   \n",
       "...      ...          ...       ...   ...     ...          ...     ...    ...   \n",
       "10951  10964            1        27    77     117         2015       2   1946   \n",
       "10952  10965            2        26    74     114         2015       2   1977   \n",
       "10953  10966            2        29    84     127         2015       2   1968   \n",
       "10954  10967            1        11    31      54         2015       1   1950   \n",
       "10955  10968            1        13    37      68         2015       1   1941   \n",
       "\n",
       "       nationality  religion  ...  public_service_7  public_service_8  \\\n",
       "0                1         1  ...                50                50   \n",
       "1                1         1  ...                90                60   \n",
       "2                1         0  ...                90                90   \n",
       "3                1         1  ...                90                80   \n",
       "4                1         1  ...                50                50   \n",
       "...            ...       ...  ...               ...               ...   \n",
       "10951            1         1  ...                40                60   \n",
       "10952            1         1  ...                50                50   \n",
       "10953            1         1  ...                60                60   \n",
       "10954            1         1  ...                80                80   \n",
       "10955            1         1  ...                90                90   \n",
       "\n",
       "      public_service_9  neg1  neg2  neg3  neg4  neg5  age  age_bin  \n",
       "0                   50     5     3     0     2     0   56        4  \n",
       "1                   60     0     0     0     0     0   23        1  \n",
       "2                   75     3     1     0     2     0   48        3  \n",
       "3                   80     2     0     0     2     0   72        5  \n",
       "4                   50     2     1     1     0     0   21        1  \n",
       "...                ...   ...   ...   ...   ...   ...  ...      ...  \n",
       "10951               50     0     0     0     0     0   69        5  \n",
       "10952               50     2     0     0     2     0   38        3  \n",
       "10953               60     8     6     0     2     0   47        3  \n",
       "10954               80     4     1     1     2     0   65        5  \n",
       "10955               90    11     9     0     2     0   74        5  \n",
       "\n",
       "[10956 rows x 146 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对‘宗教’处理\n",
    "data.loc[data['religion']<0,'religion'] = 1 #1为不信仰宗教\n",
    "data.loc[data['religion_freq']<0,'religion_freq'] = 1 #1为从来没有参加过\n",
    "#对‘教育程度’处理\n",
    "data.loc[data['edu']<0,'edu'] = 4 #初中\n",
    "data.loc[data['edu_status']<0,'edu_status'] = 0\n",
    "data.loc[data['edu_yr']<0,'edu_yr'] = 0\n",
    "#对‘个人收入’处理\n",
    "data.loc[data['income']<0,'income'] = 0 #认为无收入\n",
    "#对‘政治面貌’处理\n",
    "data.loc[data['political']<0,'political'] = 1 #认为是群众\n",
    "#对体重处理\n",
    "data.loc[(data['weight_jin']<=80)&(data['height_cm']>=160),'weight_jin']= data['weight_jin']*2\n",
    "data.loc[data['weight_jin']<=60,'weight_jin']= data['weight_jin']*2  #个人的想法，哈哈哈，没有60斤的成年人吧\n",
    "#对身高处理\n",
    "data.loc[data['height_cm']<150,'height_cm'] = 150 #成年人的实际情况\n",
    "#对‘健康’处理\n",
    "data.loc[data['health']<0,'health'] = 4 #认为是比较健康\n",
    "data.loc[data['health_problem']<0,'health_problem'] = 4\n",
    "#对‘沮丧’处理\n",
    "data.loc[data['depression']<0,'depression'] = 4 #一般人都是很少吧\n",
    "#对‘媒体’处理\n",
    "data.loc[data['media_1']<0,'media_1'] = 1 #都是从不\n",
    "data.loc[data['media_2']<0,'media_2'] = 1\n",
    "data.loc[data['media_3']<0,'media_3'] = 1\n",
    "data.loc[data['media_4']<0,'media_4'] = 1\n",
    "data.loc[data['media_5']<0,'media_5'] = 1\n",
    "data.loc[data['media_6']<0,'media_6'] = 1\n",
    "#对‘空闲活动’处理\n",
    "data.loc[data['leisure_1']<0,'leisure_1'] = 1 #都是根据自己的想法\n",
    "data.loc[data['leisure_2']<0,'leisure_2'] = 5\n",
    "data.loc[data['leisure_3']<0,'leisure_3'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.loc[data['leisure_4']<0,'leisure_4'] = data['leisure_4'].mode() #取众数\n",
    "data.loc[data['leisure_5']<0,'leisure_5'] = data['leisure_5'].mode()\n",
    "data.loc[data['leisure_6']<0,'leisure_6'] = data['leisure_6'].mode()\n",
    "data.loc[data['leisure_7']<0,'leisure_7'] = data['leisure_7'].mode()\n",
    "data.loc[data['leisure_8']<0,'leisure_8'] = data['leisure_8'].mode()\n",
    "data.loc[data['leisure_9']<0,'leisure_9'] = data['leisure_9'].mode()\n",
    "data.loc[data['leisure_10']<0,'leisure_10'] = data['leisure_10'].mode()\n",
    "data.loc[data['leisure_11']<0,'leisure_11'] = data['leisure_11'].mode()\n",
    "data.loc[data['leisure_12']<0,'leisure_12'] = data['leisure_12'].mode()\n",
    "data.loc[data['socialize']<0,'socialize'] = 2 #很少\n",
    "data.loc[data['relax']<0,'relax'] = 4 #经常\n",
    "data.loc[data['learn']<0,'learn'] = 1 #从不，哈哈哈哈\n",
    "#对‘社交’处理\n",
    "data.loc[data['social_neighbor']<0,'social_neighbor'] = 0\n",
    "data.loc[data['social_friend']<0,'social_friend'] = 0\n",
    "data.loc[data['socia_outing']<0,'socia_outing'] = 1\n",
    "data.loc[data['neighbor_familiarity']<0,'social_neighbor']= 4\n",
    "#对‘社会公平性’处理\n",
    "data.loc[data['equity']<0,'equity'] = 4\n",
    "#对‘社会等级’处理\n",
    "data.loc[data['class_10_before']<0,'class_10_before'] = 3\n",
    "data.loc[data['class']<0,'class'] = 5\n",
    "data.loc[data['class_10_after']<0,'class_10_after'] = 5\n",
    "data.loc[data['class_14']<0,'class_14'] = 2\n",
    "#对‘工作情况’处理\n",
    "data.loc[data['work_status']<0,'work_status'] = 0\n",
    "data.loc[data['work_yr']<0,'work_yr'] = 0\n",
    "data.loc[data['work_manage']<0,'work_manage'] = 0\n",
    "data.loc[data['work_type']<0,'work_type'] = 0\n",
    "#对‘社会保障’处理\n",
    "data.loc[data['insur_1']<0,'insur_1'] = 1\n",
    "data.loc[data['insur_2']<0,'insur_2'] = 1\n",
    "data.loc[data['insur_3']<0,'insur_3'] = 1\n",
    "data.loc[data['insur_4']<0,'insur_4'] = 1\n",
    "data.loc[data['insur_1']==0,'insur_1'] = 0\n",
    "data.loc[data['insur_2']==0,'insur_2'] = 0\n",
    "data.loc[data['insur_3']==0,'insur_3'] = 0\n",
    "data.loc[data['insur_4']==0,'insur_4'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对家庭情况处理\n",
    "family_income_mean = data['family_income'].mean()\n",
    "data.loc[data['family_income']<0,'family_income'] = family_income_mean\n",
    "data.loc[data['family_m']<0,'family_m'] = 2\n",
    "data.loc[data['family_status']<0,'family_status'] = 3\n",
    "data.loc[data['house']<0,'house'] = 1\n",
    "data.loc[data['car']<0,'car'] = 0\n",
    "data.loc[data['car']==2,'car'] = 0 #变为0和1\n",
    "data.loc[data['son']<0,'son'] = 1\n",
    "data.loc[data['daughter']<0,'daughter'] = 0\n",
    "data.loc[data['minor_child']<0,'minor_child'] = 0\n",
    "#对‘婚姻’处理\n",
    "data.loc[data['marital_1st']<0,'marital_1st'] = 0\n",
    "data.loc[data['marital_now']<0,'marital_now'] = 0\n",
    "#对‘配偶’处理\n",
    "data.loc[data['s_birth']<0,'s_birth'] = 0\n",
    "data.loc[data['s_edu']<0,'s_edu'] = 0\n",
    "data.loc[data['s_political']<0,'s_political'] = 0\n",
    "data.loc[data['s_hukou']<0,'s_hukou'] = 0\n",
    "data.loc[data['s_income']<0,'s_income'] = 0\n",
    "data.loc[data['s_work_type']<0,'s_work_type'] = 0\n",
    "data.loc[data['s_work_status']<0,'s_work_status'] = 0\n",
    "data.loc[data['s_work_exper']<0,'s_work_exper'] = 0\n",
    "#对‘父母情况’处理\n",
    "data.loc[data['f_birth']<0,'f_birth'] = 1945\n",
    "data.loc[data['f_edu']<0,'f_edu'] = 1\n",
    "data.loc[data['f_political']<0,'f_political'] = 1\n",
    "data.loc[data['f_work_14']<0,'f_work_14'] = 2\n",
    "data.loc[data['m_birth']<0,'m_birth'] = 1940\n",
    "data.loc[data['m_edu']<0,'m_edu'] = 1\n",
    "data.loc[data['m_political']<0,'m_political'] = 1\n",
    "data.loc[data['m_work_14']<0,'m_work_14'] = 2\n",
    "#和同龄人相比社会经济地位\n",
    "data.loc[data['status_peer']<0,'status_peer'] = 2\n",
    "#和3年前比社会经济地位\n",
    "data.loc[data['status_3_before']<0,'status_3_before'] = 2\n",
    "#对‘观点’处理\n",
    "data.loc[data['view']<0,'view'] = 4\n",
    "#对期望年收入处理\n",
    "data.loc[data['inc_ability']<=0,'inc_ability']= 2\n",
    "inc_exp_mean = data['inc_exp'].mean()\n",
    "data.loc[data['inc_exp']<=0,'inc_exp']= inc_exp_mean #取均值\n",
    "\n",
    "#部分特征处理，取众数（首先去除缺失值的数据）\n",
    "for i in range(1,9+1):\n",
    "    data.loc[data['public_service_'+str(i)]<0,'public_service_'+str(i)] = data['public_service_'+str(i)].dropna().mode().values\n",
    "for i in range(1,13+1):\n",
    "    data.loc[data['trust_'+str(i)]<0,'trust_'+str(i)] = data['trust_'+str(i)].dropna().mode().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步，我们需要进一步分析每一个特征之间的关系，从而进行数据增广。经过思考，这里我添加了如下的特征：第一次结婚年龄、最近结婚年龄、是否再婚、配偶年龄、配偶年龄差、各种收入比（与配偶之间的收入比、十年后预期收入与现在收入之比等等）、收入与住房面积比（其中也包括10年后期望收入等等各种情况）、社会阶级（10年后的社会阶级、14年后的社会阶级等等）、悠闲指数、满意指数、信任指数等等。除此之外，我还考虑了对于同一省、市、县进行了归一化。例如同一省市内的收入的平均值等以及一个个体相对于同省、市、县其他人的各个指标的情况。同时也考虑了对于同龄人之间的相互比较，即在同龄人中的收入情况、健康情况等等。具体的实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一次结婚年龄 147\n",
    "data['marital_1stbir'] = data['marital_1st'] - data['birth'] \n",
    "#最近结婚年龄 148\n",
    "data['marital_nowtbir'] = data['marital_now'] - data['birth'] \n",
    "#是否再婚 149\n",
    "data['mar'] = data['marital_nowtbir'] - data['marital_1stbir']\n",
    "#配偶年龄 150\n",
    "data['marital_sbir'] = data['marital_now']-data['s_birth']\n",
    "#配偶年龄差 151\n",
    "data['age_'] = data['marital_nowtbir'] - data['marital_sbir'] \n",
    "\n",
    "#收入比 151+7 =158\n",
    "data['income/s_income'] = data['income']/(data['s_income']+1) #同居伴侣\n",
    "data['income+s_income'] = data['income']+(data['s_income']+1)\n",
    "data['income/family_income'] = data['income']/(data['family_income']+1)\n",
    "data['all_income/family_income'] = (data['income']+data['s_income'])/(data['family_income']+1)\n",
    "data['income/inc_exp'] = data['income']/(data['inc_exp']+1)\n",
    "data['family_income/m'] = data['family_income']/(data['family_m']+0.01)\n",
    "data['income/m'] = data['income']/(data['family_m']+0.01)\n",
    "\n",
    "#收入/面积比 158+4=162\n",
    "data['income/floor_area'] = data['income']/(data['floor_area']+0.01)\n",
    "data['all_income/floor_area'] = (data['income']+data['s_income'])/(data['floor_area']+0.01)\n",
    "data['family_income/floor_area'] = data['family_income']/(data['floor_area']+0.01)\n",
    "data['floor_area/m'] = data['floor_area']/(data['family_m']+0.01)\n",
    "\n",
    "#class 162+3=165\n",
    "data['class_10_diff'] = (data['class_10_after'] - data['class'])\n",
    "data['class_diff'] = data['class'] - data['class_10_before']\n",
    "data['class_14_diff'] = data['class'] - data['class_14']\n",
    "#悠闲指数 166\n",
    "leisure_fea_lis = ['leisure_'+str(i) for i in range(1,13)]\n",
    "data['leisure_sum'] = data[leisure_fea_lis].sum(axis=1) #skew\n",
    "#满意指数 167\n",
    "public_service_fea_lis = ['public_service_'+str(i) for i in range(1,10)]\n",
    "data['public_service_sum'] = data[public_service_fea_lis].sum(axis=1) #skew\n",
    "\n",
    "#信任指数 168\n",
    "trust_fea_lis = ['trust_'+str(i) for i in range(1,14)]\n",
    "data['trust_sum'] = data[trust_fea_lis].sum(axis=1) #skew\n",
    "\n",
    "#province mean 168+13=181\n",
    "data['province_income_mean'] = data.groupby(['province'])['income'].transform('mean').values\n",
    "data['province_family_income_mean'] = data.groupby(['province'])['family_income'].transform('mean').values\n",
    "data['province_equity_mean'] = data.groupby(['province'])['equity'].transform('mean').values\n",
    "data['province_depression_mean'] = data.groupby(['province'])['depression'].transform('mean').values\n",
    "data['province_floor_area_mean'] = data.groupby(['province'])['floor_area'].transform('mean').values\n",
    "data['province_health_mean'] = data.groupby(['province'])['health'].transform('mean').values\n",
    "data['province_class_10_diff_mean'] = data.groupby(['province'])['class_10_diff'].transform('mean').values\n",
    "data['province_class_mean'] = data.groupby(['province'])['class'].transform('mean').values\n",
    "data['province_health_problem_mean'] = data.groupby(['province'])['health_problem'].transform('mean').values\n",
    "data['province_family_status_mean'] = data.groupby(['province'])['family_status'].transform('mean').values\n",
    "data['province_leisure_sum_mean'] = data.groupby(['province'])['leisure_sum'].transform('mean').values\n",
    "data['province_public_service_sum_mean'] = data.groupby(['province'])['public_service_sum'].transform('mean').values\n",
    "data['province_trust_sum_mean'] = data.groupby(['province'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#city   mean 181+13=194\n",
    "data['city_income_mean'] = data.groupby(['city'])['income'].transform('mean').values #按照city分组\n",
    "data['city_family_income_mean'] = data.groupby(['city'])['family_income'].transform('mean').values\n",
    "data['city_equity_mean'] = data.groupby(['city'])['equity'].transform('mean').values\n",
    "data['city_depression_mean'] = data.groupby(['city'])['depression'].transform('mean').values\n",
    "data['city_floor_area_mean'] = data.groupby(['city'])['floor_area'].transform('mean').values\n",
    "data['city_health_mean'] = data.groupby(['city'])['health'].transform('mean').values\n",
    "data['city_class_10_diff_mean'] = data.groupby(['city'])['class_10_diff'].transform('mean').values\n",
    "data['city_class_mean'] = data.groupby(['city'])['class'].transform('mean').values\n",
    "data['city_health_problem_mean'] = data.groupby(['city'])['health_problem'].transform('mean').values\n",
    "data['city_family_status_mean'] = data.groupby(['city'])['family_status'].transform('mean').values\n",
    "data['city_leisure_sum_mean'] = data.groupby(['city'])['leisure_sum'].transform('mean').values\n",
    "data['city_public_service_sum_mean'] = data.groupby(['city'])['public_service_sum'].transform('mean').values\n",
    "data['city_trust_sum_mean'] = data.groupby(['city'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#county  mean 194 + 13 = 207\n",
    "data['county_income_mean'] = data.groupby(['county'])['income'].transform('mean').values\n",
    "data['county_family_income_mean'] = data.groupby(['county'])['family_income'].transform('mean').values\n",
    "data['county_equity_mean'] = data.groupby(['county'])['equity'].transform('mean').values\n",
    "data['county_depression_mean'] = data.groupby(['county'])['depression'].transform('mean').values\n",
    "data['county_floor_area_mean'] = data.groupby(['county'])['floor_area'].transform('mean').values\n",
    "data['county_health_mean'] = data.groupby(['county'])['health'].transform('mean').values\n",
    "data['county_class_10_diff_mean'] = data.groupby(['county'])['class_10_diff'].transform('mean').values\n",
    "data['county_class_mean'] = data.groupby(['county'])['class'].transform('mean').values\n",
    "data['county_health_problem_mean'] = data.groupby(['county'])['health_problem'].transform('mean').values\n",
    "data['county_family_status_mean'] = data.groupby(['county'])['family_status'].transform('mean').values\n",
    "data['county_leisure_sum_mean'] = data.groupby(['county'])['leisure_sum'].transform('mean').values\n",
    "data['county_public_service_sum_mean'] = data.groupby(['county'])['public_service_sum'].transform('mean').values\n",
    "data['county_trust_sum_mean'] = data.groupby(['county'])['trust_sum'].transform('mean').values\n",
    "\n",
    "#ratio 相比同省 207 + 13 =220\n",
    "data['income/province'] = data['income']/(data['province_income_mean'])                                      \n",
    "data['family_income/province'] = data['family_income']/(data['province_family_income_mean'])   \n",
    "data['equity/province'] = data['equity']/(data['province_equity_mean'])       \n",
    "data['depression/province'] = data['depression']/(data['province_depression_mean'])                                                \n",
    "data['floor_area/province'] = data['floor_area']/(data['province_floor_area_mean'])\n",
    "data['health/province'] = data['health']/(data['province_health_mean'])\n",
    "data['class_10_diff/province'] = data['class_10_diff']/(data['province_class_10_diff_mean'])\n",
    "data['class/province'] = data['class']/(data['province_class_mean'])\n",
    "data['health_problem/province'] = data['health_problem']/(data['province_health_problem_mean'])\n",
    "data['family_status/province'] = data['family_status']/(data['province_family_status_mean'])\n",
    "data['leisure_sum/province'] = data['leisure_sum']/(data['province_leisure_sum_mean'])\n",
    "data['public_service_sum/province'] = data['public_service_sum']/(data['province_public_service_sum_mean'])\n",
    "data['trust_sum/province'] = data['trust_sum']/(data['province_trust_sum_mean']+1)\n",
    "\n",
    "#ratio 相比同市 220 + 13 =233\n",
    "data['income/city'] = data['income']/(data['city_income_mean'])                                      \n",
    "data['family_income/city'] = data['family_income']/(data['city_family_income_mean'])   \n",
    "data['equity/city'] = data['equity']/(data['city_equity_mean'])       \n",
    "data['depression/city'] = data['depression']/(data['city_depression_mean'])                                                \n",
    "data['floor_area/city'] = data['floor_area']/(data['city_floor_area_mean'])\n",
    "data['health/city'] = data['health']/(data['city_health_mean'])\n",
    "data['class_10_diff/city'] = data['class_10_diff']/(data['city_class_10_diff_mean'])\n",
    "data['class/city'] = data['class']/(data['city_class_mean'])\n",
    "data['health_problem/city'] = data['health_problem']/(data['city_health_problem_mean'])\n",
    "data['family_status/city'] = data['family_status']/(data['city_family_status_mean'])\n",
    "data['leisure_sum/city'] = data['leisure_sum']/(data['city_leisure_sum_mean'])\n",
    "data['public_service_sum/city'] = data['public_service_sum']/(data['city_public_service_sum_mean'])\n",
    "data['trust_sum/city'] = data['trust_sum']/(data['city_trust_sum_mean'])\n",
    "\n",
    "#ratio 相比同个地区 233 + 13 =246\n",
    "data['income/county'] = data['income']/(data['county_income_mean'])                                      \n",
    "data['family_income/county'] = data['family_income']/(data['county_family_income_mean'])   \n",
    "data['equity/county'] = data['equity']/(data['county_equity_mean'])       \n",
    "data['depression/county'] = data['depression']/(data['county_depression_mean'])                                                \n",
    "data['floor_area/county'] = data['floor_area']/(data['county_floor_area_mean'])\n",
    "data['health/county'] = data['health']/(data['county_health_mean'])\n",
    "data['class_10_diff/county'] = data['class_10_diff']/(data['county_class_10_diff_mean'])\n",
    "data['class/county'] = data['class']/(data['county_class_mean'])\n",
    "data['health_problem/county'] = data['health_problem']/(data['county_health_problem_mean'])\n",
    "data['family_status/county'] = data['family_status']/(data['county_family_status_mean'])\n",
    "data['leisure_sum/county'] = data['leisure_sum']/(data['county_leisure_sum_mean'])\n",
    "data['public_service_sum/county'] = data['public_service_sum']/(data['county_public_service_sum_mean'])\n",
    "data['trust_sum/county'] = data['trust_sum']/(data['county_trust_sum_mean'])\n",
    "\n",
    "#age   mean 246+ 13 =259\n",
    "data['age_income_mean'] = data.groupby(['age'])['income'].transform('mean').values\n",
    "data['age_family_income_mean'] = data.groupby(['age'])['family_income'].transform('mean').values\n",
    "data['age_equity_mean'] = data.groupby(['age'])['equity'].transform('mean').values\n",
    "data['age_depression_mean'] = data.groupby(['age'])['depression'].transform('mean').values\n",
    "data['age_floor_area_mean'] = data.groupby(['age'])['floor_area'].transform('mean').values\n",
    "data['age_health_mean'] = data.groupby(['age'])['health'].transform('mean').values\n",
    "data['age_class_10_diff_mean'] = data.groupby(['age'])['class_10_diff'].transform('mean').values\n",
    "data['age_class_mean'] = data.groupby(['age'])['class'].transform('mean').values\n",
    "data['age_health_problem_mean'] = data.groupby(['age'])['health_problem'].transform('mean').values\n",
    "data['age_family_status_mean'] = data.groupby(['age'])['family_status'].transform('mean').values\n",
    "data['age_leisure_sum_mean'] = data.groupby(['age'])['leisure_sum'].transform('mean').values\n",
    "data['age_public_service_sum_mean'] = data.groupby(['age'])['public_service_sum'].transform('mean').values\n",
    "data['age_trust_sum_mean'] = data.groupby(['age'])['trust_sum'].transform('mean').values\n",
    "\n",
    "# 和同龄人相比259 + 13 =272\n",
    "data['income/age'] = data['income']/(data['age_income_mean'])                                      \n",
    "data['family_income/age'] = data['family_income']/(data['age_family_income_mean'])   \n",
    "data['equity/age'] = data['equity']/(data['age_equity_mean'])       \n",
    "data['depression/age'] = data['depression']/(data['age_depression_mean'])                                                \n",
    "data['floor_area/age'] = data['floor_area']/(data['age_floor_area_mean'])\n",
    "data['health/age'] = data['health']/(data['age_health_mean'])\n",
    "data['class_10_diff/age'] = data['class_10_diff']/(data['age_class_10_diff_mean'])\n",
    "data['class/age'] = data['class']/(data['age_class_mean'])\n",
    "data['health_problem/age'] = data['health_problem']/(data['age_health_problem_mean'])\n",
    "data['family_status/age'] = data['family_status']/(data['age_family_status_mean'])\n",
    "data['leisure_sum/age'] = data['leisure_sum']/(data['age_leisure_sum_mean'])\n",
    "data['public_service_sum/age'] = data['public_service_sum']/(data['age_public_service_sum_mean'])\n",
    "data['trust_sum/age'] = data['trust_sum']/(data['age_trust_sum_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (10956, 272)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>survey_type</th>\n",
       "      <th>province</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>survey_time</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>religion</th>\n",
       "      <th>...</th>\n",
       "      <th>depression/age</th>\n",
       "      <th>floor_area/age</th>\n",
       "      <th>health/age</th>\n",
       "      <th>class_10_diff/age</th>\n",
       "      <th>class/age</th>\n",
       "      <th>health_problem/age</th>\n",
       "      <th>family_status/age</th>\n",
       "      <th>leisure_sum/age</th>\n",
       "      <th>public_service_sum/age</th>\n",
       "      <th>trust_sum/age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1959</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.285211</td>\n",
       "      <td>0.410351</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683307</td>\n",
       "      <td>0.521429</td>\n",
       "      <td>0.733668</td>\n",
       "      <td>0.724620</td>\n",
       "      <td>0.666638</td>\n",
       "      <td>0.925941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.952824</td>\n",
       "      <td>1.179337</td>\n",
       "      <td>1.012552</td>\n",
       "      <td>1.344444</td>\n",
       "      <td>0.891344</td>\n",
       "      <td>1.359551</td>\n",
       "      <td>1.011792</td>\n",
       "      <td>1.130778</td>\n",
       "      <td>1.188442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>126</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.343537</td>\n",
       "      <td>0.972328</td>\n",
       "      <td>1.150485</td>\n",
       "      <td>1.190955</td>\n",
       "      <td>1.195762</td>\n",
       "      <td>1.055679</td>\n",
       "      <td>1.190955</td>\n",
       "      <td>0.966470</td>\n",
       "      <td>1.193204</td>\n",
       "      <td>0.803693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.111663</td>\n",
       "      <td>0.642329</td>\n",
       "      <td>1.276353</td>\n",
       "      <td>4.977778</td>\n",
       "      <td>1.199143</td>\n",
       "      <td>1.188329</td>\n",
       "      <td>1.162630</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>1.153810</td>\n",
       "      <td>1.300950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.587284</td>\n",
       "      <td>1.177106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236957</td>\n",
       "      <td>1.116803</td>\n",
       "      <td>1.093645</td>\n",
       "      <td>1.045313</td>\n",
       "      <td>0.728161</td>\n",
       "      <td>1.117428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  survey_type  province  city  county  survey_time  gender  birth  \\\n",
       "0   1            1        12    32      59         2015       1   1959   \n",
       "1   2            2        18    52      85         2015       1   1992   \n",
       "2   3            2        29    83     126         2015       2   1967   \n",
       "3   4            2        10    28      51         2015       2   1943   \n",
       "4   5            1         7    18      36         2015       2   1994   \n",
       "\n",
       "   nationality  religion  ...  depression/age  floor_area/age health/age  \\\n",
       "0            1         1  ...        1.285211        0.410351   0.848837   \n",
       "1            1         1  ...        0.733333        0.952824   1.179337   \n",
       "2            1         0  ...        1.343537        0.972328   1.150485   \n",
       "3            1         1  ...        1.111663        0.642329   1.276353   \n",
       "4            1         1  ...        0.750000        0.587284   1.177106   \n",
       "\n",
       "   class_10_diff/age  class/age  health_problem/age  family_status/age  \\\n",
       "0           0.000000   0.683307            0.521429           0.733668   \n",
       "1           1.012552   1.344444            0.891344           1.359551   \n",
       "2           1.190955   1.195762            1.055679           1.190955   \n",
       "3           4.977778   1.199143            1.188329           1.162630   \n",
       "4           0.000000   0.236957            1.116803           1.093645   \n",
       "\n",
       "   leisure_sum/age  public_service_sum/age  trust_sum/age  \n",
       "0         0.724620                0.666638       0.925941  \n",
       "1         1.011792                1.130778       1.188442  \n",
       "2         0.966470                1.193204       0.803693  \n",
       "3         0.899346                1.153810       1.300950  \n",
       "4         1.045313                0.728161       1.117428  \n",
       "\n",
       "[5 rows x 272 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape',data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据从131维扩充到272维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删去有效样本数很少的特征，例如负值太多的特征或者是缺失值太多的特征，这里我一共删除了包括“目前的最高教育程度”在内的9类特征，得到了最终的263维的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 263)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#272-9=263\n",
    "#删除数值特别少的和之前用过的特征\n",
    "del_list=['id','survey_time','edu_other','invest_other','property_other','join_party','province','city','county']\n",
    "use_feature = [clo for clo in data.columns if clo not in del_list]\n",
    "data.fillna(0,inplace=True) #还是补0\n",
    "train_shape = train.shape[0] #一共的数据量，训练集\n",
    "features = data[use_feature].columns #删除后所有的特征\n",
    "X_train_263 = data[:train_shape][use_feature].values\n",
    "y_train = target\n",
    "X_test_263 = data[train_shape:][use_feature].values\n",
    "X_train_263.shape #最终一种263个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择了最重要的49个特征，作为除了以上263维特征外的另外一组特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 49)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_fea_49 = ['equity','depression','health','class','family_status','health_problem','class_10_after',\n",
    "           'equity/province','equity/city','equity/county',\n",
    "           'depression/province','depression/city','depression/county',\n",
    "           'health/province','health/city','health/county',\n",
    "           'class/province','class/city','class/county',\n",
    "           'family_status/province','family_status/city','family_status/county',\n",
    "           'family_income/province','family_income/city','family_income/county',\n",
    "           'floor_area/province','floor_area/city','floor_area/county',\n",
    "           'leisure_sum/province','leisure_sum/city','leisure_sum/county',\n",
    "           'public_service_sum/province','public_service_sum/city','public_service_sum/county',\n",
    "           'trust_sum/province','trust_sum/city','trust_sum/county',\n",
    "           'income/m','public_service_sum','class_diff','status_3_before','age_income_mean','age_floor_area_mean',\n",
    "           'weight_jin','height_cm',\n",
    "           'health/age','depression/age','equity/age','leisure_sum/age'\n",
    "          ]\n",
    "train_shape = train.shape[0]\n",
    "X_train_49 = data[:train_shape][imp_fea_49].values\n",
    "X_test_49 = data[train_shape:][imp_fea_49].values\n",
    "X_train_49.shape #最重要的49个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择需要进行onehot编码的离散变量进行one-hot编码，再合成为第三类特征，共383维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 383)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_fea = ['survey_type','gender','nationality','edu_status','political','hukou','hukou_loc','work_exper','work_status','work_type',\n",
    "           'work_manage','marital','s_political','s_hukou','s_work_exper','s_work_status','s_work_type','f_political','f_work_14',\n",
    "           'm_political','m_work_14'] #已经是0、1的值不需要onehot\n",
    "noc_fea = [clo for clo in use_feature if clo not in cat_fea]\n",
    "\n",
    "onehot_data = data[cat_fea].values\n",
    "enc = preprocessing.OneHotEncoder(categories = 'auto')\n",
    "oh_data=enc.fit_transform(onehot_data).toarray()\n",
    "oh_data.shape #变为onehot编码格式\n",
    "\n",
    "X_train_oh = oh_data[:train_shape,:]\n",
    "X_test_oh = oh_data[train_shape:,:]\n",
    "X_train_oh.shape #其中的训练集\n",
    "\n",
    "X_train_383 = np.column_stack([data[:train_shape][noc_fea].values,X_train_oh])#先是noc，再是cat_fea\n",
    "X_test_383 = np.column_stack([data[train_shape:][noc_fea].values,X_test_oh])\n",
    "X_train_383.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7988, 383)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_fea = ['survey_type','gender','nationality','edu_status','political','hukou','hukou_loc','work_exper','work_status','work_type',\n",
    "           'work_manage','marital','s_political','s_hukou','s_work_exper','s_work_status','s_work_type','f_political','f_work_14',\n",
    "           'm_political','m_work_14'] #已经是0、1的值不需要onehot\n",
    "noc_fea = [clo for clo in use_feature if clo not in cat_fea]\n",
    "\n",
    "onehot_data = data[cat_fea].values\n",
    "enc = preprocessing.OneHotEncoder(categories = 'auto')\n",
    "oh_data=enc.fit_transform(onehot_data).toarray()\n",
    "oh_data.shape #变为onehot编码格式\n",
    "\n",
    "X_train_oh = oh_data[:train_shape,:]\n",
    "X_test_oh = oh_data[train_shape:,:]\n",
    "X_train_oh.shape #其中的训练集\n",
    "\n",
    "X_train_383 = np.column_stack([data[:train_shape][noc_fea].values,X_train_oh])#先是noc，再是cat_fea\n",
    "X_test_383 = np.column_stack([data[train_shape:][noc_fea].values,X_test_oh])\n",
    "X_train_383.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1959, 1, 1, ..., 0.0, 0.0, 0.0],\n",
       "       [1992, 1, 1, ..., 0.0, 0.0, 0.0],\n",
       "       [1967, 0, 3, ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       [1967, 1, 1, ..., 0.0, 0.0, 0.0],\n",
       "       [1978, 1, 1, ..., 0.0, 0.0, 0.0],\n",
       "       [1991, 1, 1, ..., 1.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于此，我们构建完成了三种特征工程（训练数据集），其一是上面提取的最重要的49中特征，其中包括健康程度、社会阶级、在同龄人中的收入情况等等特征。其二是扩充后的263维特征（这里可以认为是初始特征）。其三是使用One-hot编码后的特征，这里要使用One-hot进行编码的原因在于，有部分特征为分离值，例如性别中男女，男为1，女为2，我们想使用One-hot将其变为男为0，女为1，来增强机器学习算法的鲁棒性能；再如民族这个特征，原本是1-56这56个数值，如果直接分类会让分类器的鲁棒性变差，所以使用One-hot编码将其变为6个特征进行非零即一的处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于每一类模型，使用stacking(K-fold)方法去建构第一层模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先我们对于原始的263维的特征，使用**lightGBM**进行处理，这里我们使用5折交叉验证的方法:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.499759\tvalid_1's l2: 0.532511\n",
      "[1000]\ttraining's l2: 0.451528\tvalid_1's l2: 0.499127\n",
      "[1500]\ttraining's l2: 0.425443\tvalid_1's l2: 0.485366\n",
      "[2000]\ttraining's l2: 0.407389\tvalid_1's l2: 0.479308\n",
      "[2500]\ttraining's l2: 0.393001\tvalid_1's l2: 0.475557\n",
      "[3000]\ttraining's l2: 0.380766\tvalid_1's l2: 0.473685\n",
      "[3500]\ttraining's l2: 0.370009\tvalid_1's l2: 0.47256\n",
      "[4000]\ttraining's l2: 0.36022\tvalid_1's l2: 0.471582\n",
      "[4500]\ttraining's l2: 0.35124\tvalid_1's l2: 0.470863\n",
      "[5000]\ttraining's l2: 0.342828\tvalid_1's l2: 0.470557\n",
      "[5500]\ttraining's l2: 0.334901\tvalid_1's l2: 0.470027\n",
      "[6000]\ttraining's l2: 0.327384\tvalid_1's l2: 0.469966\n",
      "[6500]\ttraining's l2: 0.320155\tvalid_1's l2: 0.469936\n",
      "Early stopping, best iteration is:\n",
      "[6113]\ttraining's l2: 0.325706\tvalid_1's l2: 0.469874\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.504322\tvalid_1's l2: 0.513628\n",
      "[1000]\ttraining's l2: 0.454889\tvalid_1's l2: 0.47926\n",
      "[1500]\ttraining's l2: 0.428782\tvalid_1's l2: 0.465975\n",
      "[2000]\ttraining's l2: 0.410927\tvalid_1's l2: 0.459213\n",
      "[2500]\ttraining's l2: 0.397259\tvalid_1's l2: 0.455058\n",
      "[3000]\ttraining's l2: 0.385427\tvalid_1's l2: 0.45243\n",
      "[3500]\ttraining's l2: 0.374844\tvalid_1's l2: 0.45074\n",
      "[4000]\ttraining's l2: 0.365255\tvalid_1's l2: 0.449348\n",
      "[4500]\ttraining's l2: 0.356344\tvalid_1's l2: 0.44843\n",
      "[5000]\ttraining's l2: 0.348003\tvalid_1's l2: 0.447449\n",
      "[5500]\ttraining's l2: 0.33999\tvalid_1's l2: 0.446694\n",
      "[6000]\ttraining's l2: 0.332323\tvalid_1's l2: 0.446199\n",
      "[6500]\ttraining's l2: 0.325038\tvalid_1's l2: 0.445977\n",
      "[7000]\ttraining's l2: 0.318133\tvalid_1's l2: 0.445744\n",
      "[7500]\ttraining's l2: 0.31142\tvalid_1's l2: 0.445212\n",
      "[8000]\ttraining's l2: 0.305112\tvalid_1's l2: 0.444905\n",
      "[8500]\ttraining's l2: 0.298968\tvalid_1's l2: 0.444798\n",
      "[9000]\ttraining's l2: 0.292984\tvalid_1's l2: 0.444621\n",
      "[9500]\ttraining's l2: 0.287211\tvalid_1's l2: 0.444357\n",
      "[10000]\ttraining's l2: 0.281666\tvalid_1's l2: 0.444061\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l2: 0.281666\tvalid_1's l2: 0.444061\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.503169\tvalid_1's l2: 0.518027\n",
      "[1000]\ttraining's l2: 0.455064\tvalid_1's l2: 0.480541\n",
      "[1500]\ttraining's l2: 0.429866\tvalid_1's l2: 0.464074\n",
      "[2000]\ttraining's l2: 0.412418\tvalid_1's l2: 0.455414\n",
      "[2500]\ttraining's l2: 0.398189\tvalid_1's l2: 0.449861\n",
      "[3000]\ttraining's l2: 0.386279\tvalid_1's l2: 0.446579\n",
      "[3500]\ttraining's l2: 0.375504\tvalid_1's l2: 0.444652\n",
      "[4000]\ttraining's l2: 0.365712\tvalid_1's l2: 0.442949\n",
      "[4500]\ttraining's l2: 0.356724\tvalid_1's l2: 0.442167\n",
      "[5000]\ttraining's l2: 0.348312\tvalid_1's l2: 0.441584\n",
      "[5500]\ttraining's l2: 0.340196\tvalid_1's l2: 0.440885\n",
      "[6000]\ttraining's l2: 0.332524\tvalid_1's l2: 0.44055\n",
      "[6500]\ttraining's l2: 0.325144\tvalid_1's l2: 0.440291\n",
      "[7000]\ttraining's l2: 0.318183\tvalid_1's l2: 0.440397\n",
      "Early stopping, best iteration is:\n",
      "[6645]\ttraining's l2: 0.323057\tvalid_1's l2: 0.440129\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.504278\tvalid_1's l2: 0.512194\n",
      "[1000]\ttraining's l2: 0.455536\tvalid_1's l2: 0.477492\n",
      "[1500]\ttraining's l2: 0.429192\tvalid_1's l2: 0.465315\n",
      "[2000]\ttraining's l2: 0.411059\tvalid_1's l2: 0.459404\n",
      "[2500]\ttraining's l2: 0.396757\tvalid_1's l2: 0.45599\n",
      "[3000]\ttraining's l2: 0.384704\tvalid_1's l2: 0.453798\n",
      "[3500]\ttraining's l2: 0.374064\tvalid_1's l2: 0.452262\n",
      "[4000]\ttraining's l2: 0.364262\tvalid_1's l2: 0.451165\n",
      "[4500]\ttraining's l2: 0.355223\tvalid_1's l2: 0.450323\n",
      "[5000]\ttraining's l2: 0.346783\tvalid_1's l2: 0.449734\n",
      "[5500]\ttraining's l2: 0.338894\tvalid_1's l2: 0.449127\n",
      "[6000]\ttraining's l2: 0.33135\tvalid_1's l2: 0.448862\n",
      "[6500]\ttraining's l2: 0.324024\tvalid_1's l2: 0.448603\n",
      "[7000]\ttraining's l2: 0.317218\tvalid_1's l2: 0.448322\n",
      "[7500]\ttraining's l2: 0.31059\tvalid_1's l2: 0.448333\n",
      "Early stopping, best iteration is:\n",
      "[7131]\ttraining's l2: 0.315453\tvalid_1's l2: 0.448191\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's l2: 0.503075\tvalid_1's l2: 0.519874\n",
      "[1000]\ttraining's l2: 0.454635\tvalid_1's l2: 0.484866\n",
      "[1500]\ttraining's l2: 0.428716\tvalid_1's l2: 0.47116\n",
      "[2000]\ttraining's l2: 0.410711\tvalid_1's l2: 0.465009\n",
      "[2500]\ttraining's l2: 0.39625\tvalid_1's l2: 0.461581\n",
      "[3000]\ttraining's l2: 0.383981\tvalid_1's l2: 0.459275\n",
      "[3500]\ttraining's l2: 0.372984\tvalid_1's l2: 0.45812\n",
      "[4000]\ttraining's l2: 0.362999\tvalid_1's l2: 0.457491\n",
      "[4500]\ttraining's l2: 0.35375\tvalid_1's l2: 0.45733\n",
      "[5000]\ttraining's l2: 0.345103\tvalid_1's l2: 0.457355\n",
      "[5500]\ttraining's l2: 0.337024\tvalid_1's l2: 0.457021\n",
      "[6000]\ttraining's l2: 0.329495\tvalid_1's l2: 0.457016\n",
      "[6500]\ttraining's l2: 0.322068\tvalid_1's l2: 0.457134\n",
      "Early stopping, best iteration is:\n",
      "[5850]\ttraining's l2: 0.331719\tvalid_1's l2: 0.456903\n",
      "CV score: 0.45183157\n"
     ]
    }
   ],
   "source": [
    "##### lgb_263 #\n",
    "#lightGBM决策树\n",
    "lgb_263_param = {\n",
    "'num_leaves': 7, \n",
    "'min_data_in_leaf': 20, #叶子可能具有的最小记录数\n",
    "'objective':'regression',\n",
    "'max_depth': -1,\n",
    "'learning_rate': 0.003,\n",
    "\"boosting\": \"gbdt\", #用gbdt算法\n",
    "\"feature_fraction\": 0.18, #例如 0.18时，意味着在每次迭代中随机选择18％的参数来建树\n",
    "\"bagging_freq\": 1,\n",
    "\"bagging_fraction\": 0.55, #每次迭代时用的数据比例\n",
    "\"bagging_seed\": 14,\n",
    "\"metric\": 'mse',\n",
    "\"lambda_l1\": 0.1,\n",
    "\"lambda_l2\": 0.2, \n",
    "\"verbosity\": -1}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)   #交叉切分：5\n",
    "oof_lgb_263 = np.zeros(len(X_train_263))\n",
    "predictions_lgb_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train_263[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train_263[val_idx], y_train[val_idx])#train:val=4:1\n",
    "\n",
    "    num_round = 10000\n",
    "    lgb_263 = lgb.train(lgb_263_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 800)\n",
    "    oof_lgb_263[val_idx] = lgb_263.predict(X_train_263[val_idx], num_iteration=lgb_263.best_iteration)\n",
    "    predictions_lgb_263 += lgb_263.predict(X_test_263, num_iteration=lgb_263.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我使用已经训练完的lightGBM的模型进行特征重要性的判断以及可视化，从结果我们可以看出，排在重要性第一位的是health/age，就是同龄人中的健康程度，与我们主观的看法基本一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x2016 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---------------特征重要性\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "df = pd.DataFrame(data[use_feature].columns.tolist(), columns=['feature'])\n",
    "df['importance']=list(lgb_263.feature_importance())\n",
    "df = df.sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(14,28))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df.head(50))\n",
    "plt.title('Features importance (averaged/folds)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用常见的机器学习方法，对于263维特征进行建模："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[22:30:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:30:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39998\tvalid_data-rmse:3.40012\n",
      "[500]\ttrain-rmse:0.40684\tvalid_data-rmse:0.68586\n",
      "[1000]\ttrain-rmse:0.27447\tvalid_data-rmse:0.68726\n",
      "[1180]\ttrain-rmse:0.23710\tvalid_data-rmse:0.68750\n",
      "fold n°2\n",
      "[22:31:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40000\tvalid_data-rmse:3.40010\n",
      "[500]\ttrain-rmse:0.41147\tvalid_data-rmse:0.67776\n",
      "[1000]\ttrain-rmse:0.27467\tvalid_data-rmse:0.67695\n",
      "[1412]\ttrain-rmse:0.19723\tvalid_data-rmse:0.67921\n",
      "fold n°3\n",
      "[22:31:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40001\tvalid_data-rmse:3.40035\n",
      "[500]\ttrain-rmse:0.40949\tvalid_data-rmse:0.67102\n",
      "[1000]\ttrain-rmse:0.27413\tvalid_data-rmse:0.67200\n",
      "[1155]\ttrain-rmse:0.24179\tvalid_data-rmse:0.67261\n",
      "fold n°4\n",
      "[22:31:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40003\tvalid_data-rmse:3.40045\n",
      "[500]\ttrain-rmse:0.40928\tvalid_data-rmse:0.66992\n",
      "[1000]\ttrain-rmse:0.27661\tvalid_data-rmse:0.67019\n",
      "[1246]\ttrain-rmse:0.22813\tvalid_data-rmse:0.67058\n",
      "fold n°5\n",
      "[22:31:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[22:31:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40024\tvalid_data-rmse:3.39999\n",
      "[500]\ttrain-rmse:0.40821\tvalid_data-rmse:0.66303\n",
      "[1000]\ttrain-rmse:0.27369\tvalid_data-rmse:0.66565\n",
      "[1075]\ttrain-rmse:0.25714\tvalid_data-rmse:0.66587\n",
      "CV score: 0.45235743\n"
     ]
    }
   ],
   "source": [
    "##### xgb_263\n",
    "#xgboost\n",
    "xgb_263_params = {'eta': 0.02,  #lr\n",
    "              'max_depth': 6,  \n",
    "              'min_child_weight':3,#最小叶子节点样本权重和\n",
    "              'gamma':0, #指定节点分裂所需的最小损失函数下降值。\n",
    "              'subsample': 0.7,  #控制对于每棵树，随机采样的比例\n",
    "              'colsample_bytree': 0.3,  #用来控制每棵随机采样的列数的占比 (每一列是一个特征)。\n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_263 = np.zeros(len(X_train_263))\n",
    "predictions_xgb_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train_263[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train_263[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_263 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_263_params)\n",
    "    oof_xgb_263[val_idx] = xgb_263.predict(xgb.DMatrix(X_train_263[val_idx]), ntree_limit=xgb_263.best_ntree_limit)\n",
    "    predictions_xgb_263 += xgb_263.predict(xgb.DMatrix(X_test_263), ntree_limit=xgb_263.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RandomForestRegressor随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   15.4s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   15.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   14.7s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   15.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:   14.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.47821594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#RandomForestRegressor随机森林\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_rfr_263 = np.zeros(len(X_train_263))\n",
    "predictions_rfr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    rfr_263 = rfr(n_estimators=1600,max_depth=9, min_samples_leaf=9, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.25,verbose=1,n_jobs=-1) #并行化\n",
    "    #verbose = 0 为不在标准输出流输出日志信息\n",
    "#verbose = 1 为输出进度条记录\n",
    "#verbose = 2 为每个epoch输出一行记录\n",
    "    rfr_263.fit(tr_x,tr_y)\n",
    "    oof_rfr_263[val_idx] = rfr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_rfr_263 += rfr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_rfr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. GradientBoostingRegressor梯度提升决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6501           0.0034           20.75s\n",
      "         2           0.6744           0.0032           20.50s\n",
      "         3           0.6503           0.0034           21.44s\n",
      "         4           0.6496           0.0030           20.89s\n",
      "         5           0.6422           0.0028           20.78s\n",
      "         6           0.6417           0.0032           20.70s\n",
      "         7           0.6413           0.0031           20.49s\n",
      "         8           0.6147           0.0032           20.58s\n",
      "         9           0.6338           0.0028           20.33s\n",
      "        10           0.6147           0.0028           20.36s\n",
      "        20           0.5914           0.0024           19.28s\n",
      "        30           0.5479           0.0020           18.66s\n",
      "        40           0.5552           0.0016           18.00s\n",
      "        50           0.5106           0.0014           17.50s\n",
      "        60           0.4958           0.0011           17.01s\n",
      "        70           0.4867           0.0009           16.48s\n",
      "        80           0.4623           0.0010           16.08s\n",
      "        90           0.4430           0.0008           15.72s\n",
      "       100           0.4213           0.0007           15.19s\n",
      "       200           0.3397           0.0001           10.11s\n",
      "       300           0.3019          -0.0000            5.00s\n",
      "       400           0.2717          -0.0000            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6521           0.0037           19.39s\n",
      "         2           0.6745           0.0031           21.22s\n",
      "         3           0.6663           0.0034           21.01s\n",
      "         4           0.6357           0.0031           20.57s\n",
      "         5           0.6410           0.0035           20.46s\n",
      "         6           0.6516           0.0032           20.49s\n",
      "         7           0.6485           0.0030           20.37s\n",
      "         8           0.6463           0.0030           20.18s\n",
      "         9           0.6255           0.0028           20.07s\n",
      "        10           0.6326           0.0027           20.08s\n",
      "        20           0.5935           0.0022           19.13s\n",
      "        30           0.5578           0.0018           18.38s\n",
      "        40           0.5297           0.0018           18.40s\n",
      "        50           0.5037           0.0016           18.14s\n",
      "        60           0.4890           0.0009           17.79s\n",
      "        70           0.4888           0.0009           17.17s\n",
      "        80           0.4590           0.0007           16.70s\n",
      "        90           0.4395           0.0006           16.14s\n",
      "       100           0.4210           0.0006           15.68s\n",
      "       200           0.3414           0.0001           10.31s\n",
      "       300           0.3030           0.0000            5.05s\n",
      "       400           0.2604          -0.0001            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6651           0.0036           21.16s\n",
      "         2           0.6544           0.0032           20.53s\n",
      "         3           0.6411           0.0033           20.02s\n",
      "         4           0.6598           0.0030           19.96s\n",
      "         5           0.6309           0.0033           20.11s\n",
      "         6           0.6358           0.0036           20.81s\n",
      "         7           0.6397           0.0033           21.24s\n",
      "         8           0.6488           0.0030           21.18s\n",
      "         9           0.6248           0.0030           20.89s\n",
      "        10           0.6143           0.0029           20.69s\n",
      "        20           0.5916           0.0023           19.94s\n",
      "        30           0.5657           0.0022           19.25s\n",
      "        40           0.5488           0.0020           18.50s\n",
      "        50           0.4993           0.0014           17.85s\n",
      "        60           0.4845           0.0013           17.25s\n",
      "        70           0.4774           0.0010           16.70s\n",
      "        80           0.4516           0.0008           16.11s\n",
      "        90           0.4349           0.0007           15.67s\n",
      "       100           0.4300           0.0005           15.19s\n",
      "       200           0.3334           0.0002           10.20s\n",
      "       300           0.2969          -0.0000            5.01s\n",
      "       400           0.2646          -0.0000            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6827           0.0029           19.55s\n",
      "         2           0.6613           0.0036           20.10s\n",
      "         3           0.6545           0.0033           19.46s\n",
      "         4           0.6447           0.0032           19.11s\n",
      "         5           0.6378           0.0034           19.12s\n",
      "         6           0.6478           0.0030           19.24s\n",
      "         7           0.6522           0.0029           19.20s\n",
      "         8           0.6302           0.0030           19.06s\n",
      "         9           0.6228           0.0028           18.94s\n",
      "        10           0.6305           0.0031           18.80s\n",
      "        20           0.5890           0.0024           18.24s\n",
      "        30           0.5517           0.0022           17.96s\n",
      "        40           0.5354           0.0018           17.41s\n",
      "        50           0.5046           0.0016           16.87s\n",
      "        60           0.4819           0.0013           16.32s\n",
      "        70           0.4802           0.0011           15.82s\n",
      "        80           0.4534           0.0008           15.30s\n",
      "        90           0.4465           0.0006           14.82s\n",
      "       100           0.4281           0.0006           14.32s\n",
      "       200           0.3467           0.0002            9.46s\n",
      "       300           0.3024           0.0001            4.74s\n",
      "       400           0.2804           0.0000            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6722           0.0031           20.75s\n",
      "         2           0.6812           0.0033           20.29s\n",
      "         3           0.6354           0.0033           19.85s\n",
      "         4           0.6688           0.0032           19.50s\n",
      "         5           0.6453           0.0031           19.28s\n",
      "         6           0.6411           0.0030           19.37s\n",
      "         7           0.6512           0.0029           19.26s\n",
      "         8           0.6325           0.0029           19.11s\n",
      "         9           0.6292           0.0032           18.94s\n",
      "        10           0.6262           0.0029           18.88s\n",
      "        20           0.5958           0.0028           18.35s\n",
      "        30           0.5678           0.0021           17.76s\n",
      "        40           0.5347           0.0017           17.20s\n",
      "        50           0.5118           0.0014           16.67s\n",
      "        60           0.4847           0.0013           16.18s\n",
      "        70           0.4689           0.0010           15.69s\n",
      "        80           0.4638           0.0008           15.21s\n",
      "        90           0.4443           0.0008           14.73s\n",
      "       100           0.4262           0.0006           14.27s\n",
      "       200           0.3442           0.0001            9.46s\n",
      "       300           0.3037           0.0000            4.75s\n",
      "       400           0.2653          -0.0001            0.00s\n",
      "CV score: 0.45742644\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingRegressor梯度提升决策树\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr_263 = np.zeros(train_shape)\n",
    "predictions_gbr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    gbr_263 = gbr(n_estimators=400, learning_rate=0.01,subsample=0.65,max_depth=7, min_samples_leaf=20,\n",
    "            max_features=0.22,verbose=1)\n",
    "    gbr_263.fit(tr_x,tr_y)\n",
    "    oof_gbr_263[val_idx] = gbr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_gbr_263 += gbr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ExtraTreesRegressor 极端随机森林回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.48629243\n"
     ]
    }
   ],
   "source": [
    "#ExtraTreesRegressor 极端随机森林回归\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_etr_263 = np.zeros(train_shape)\n",
    "predictions_etr_263 = np.zeros(len(X_test_263))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_263[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    etr_263 = etr(n_estimators=1000,max_depth=8, min_samples_leaf=12, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.4,verbose=1,n_jobs=-1)# max_feature：划分时考虑的最大特征数\n",
    "    etr_263.fit(tr_x,tr_y)\n",
    "    oof_etr_263[val_idx] = etr_263.predict(X_train_263[val_idx])\n",
    "    \n",
    "    predictions_etr_263 += etr_263.predict(X_test_263) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_etr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上5种模型的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里使用Stacking方法去建构第二层模型-Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44850136412439906"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack2 = np.vstack([oof_lgb_263,oof_xgb_263,oof_gbr_263,oof_rfr_263,oof_etr_263]).transpose()\n",
    "# transpose()函数的作用就是调换x,y,z的位置,也就是数组的索引值\n",
    "test_stack2 = np.vstack([predictions_lgb_263, predictions_xgb_263,predictions_gbr_263,predictions_rfr_263,predictions_etr_263]).transpose()\n",
    "\n",
    "#交叉验证:5折，重复2次\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack2 = np.zeros(train_stack2.shape[0])\n",
    "predictions_lr2 = np.zeros(test_stack2.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack2,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack2[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack2[val_idx], target.iloc[val_idx].values\n",
    "    #Kernel Ridge Regression\n",
    "    lr2 = kr()\n",
    "    lr2.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack2[val_idx] = lr2.predict(val_data)\n",
    "    predictions_lr2 += lr2.predict(test_stack2) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们对于49维的数据进行与上述263维数据相同的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.471019\tvalid_1's l2: 0.494326\n",
      "[2000]\ttraining's l2: 0.430666\tvalid_1's l2: 0.474748\n",
      "[3000]\ttraining's l2: 0.408207\tvalid_1's l2: 0.469893\n",
      "[4000]\ttraining's l2: 0.390556\tvalid_1's l2: 0.46828\n",
      "[5000]\ttraining's l2: 0.375144\tvalid_1's l2: 0.46747\n",
      "[6000]\ttraining's l2: 0.361309\tvalid_1's l2: 0.467255\n",
      "Early stopping, best iteration is:\n",
      "[5457]\ttraining's l2: 0.368666\tvalid_1's l2: 0.467017\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.471402\tvalid_1's l2: 0.493843\n",
      "[2000]\ttraining's l2: 0.43052\tvalid_1's l2: 0.474481\n",
      "[3000]\ttraining's l2: 0.407758\tvalid_1's l2: 0.469914\n",
      "[4000]\ttraining's l2: 0.389637\tvalid_1's l2: 0.46848\n",
      "[5000]\ttraining's l2: 0.374293\tvalid_1's l2: 0.467984\n",
      "[6000]\ttraining's l2: 0.360368\tvalid_1's l2: 0.46791\n",
      "[7000]\ttraining's l2: 0.347628\tvalid_1's l2: 0.468419\n",
      "Early stopping, best iteration is:\n",
      "[6259]\ttraining's l2: 0.356974\tvalid_1's l2: 0.467782\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.474022\tvalid_1's l2: 0.487747\n",
      "[2000]\ttraining's l2: 0.433151\tvalid_1's l2: 0.464339\n",
      "[3000]\ttraining's l2: 0.410082\tvalid_1's l2: 0.460623\n",
      "[4000]\ttraining's l2: 0.392208\tvalid_1's l2: 0.460245\n",
      "Early stopping, best iteration is:\n",
      "[3588]\ttraining's l2: 0.399063\tvalid_1's l2: 0.459916\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.467292\tvalid_1's l2: 0.508077\n",
      "[2000]\ttraining's l2: 0.427587\tvalid_1's l2: 0.489703\n",
      "[3000]\ttraining's l2: 0.405916\tvalid_1's l2: 0.48449\n",
      "[4000]\ttraining's l2: 0.388609\tvalid_1's l2: 0.48129\n",
      "[5000]\ttraining's l2: 0.373794\tvalid_1's l2: 0.479316\n",
      "[6000]\ttraining's l2: 0.360553\tvalid_1's l2: 0.477517\n",
      "[7000]\ttraining's l2: 0.348442\tvalid_1's l2: 0.476095\n",
      "[8000]\ttraining's l2: 0.337103\tvalid_1's l2: 0.474862\n",
      "[9000]\ttraining's l2: 0.326261\tvalid_1's l2: 0.474065\n",
      "[10000]\ttraining's l2: 0.31619\tvalid_1's l2: 0.47324\n",
      "[11000]\ttraining's l2: 0.306605\tvalid_1's l2: 0.47275\n",
      "[12000]\ttraining's l2: 0.297541\tvalid_1's l2: 0.472706\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\ttraining's l2: 0.297541\tvalid_1's l2: 0.472706\n",
      "fold n°5\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[1000]\ttraining's l2: 0.468538\tvalid_1's l2: 0.504093\n",
      "[2000]\ttraining's l2: 0.428006\tvalid_1's l2: 0.486731\n",
      "[3000]\ttraining's l2: 0.405601\tvalid_1's l2: 0.482439\n",
      "[4000]\ttraining's l2: 0.387814\tvalid_1's l2: 0.480658\n",
      "[5000]\ttraining's l2: 0.372535\tvalid_1's l2: 0.479483\n",
      "[6000]\ttraining's l2: 0.358699\tvalid_1's l2: 0.479134\n",
      "[7000]\ttraining's l2: 0.346094\tvalid_1's l2: 0.479209\n",
      "Early stopping, best iteration is:\n",
      "[6272]\ttraining's l2: 0.355147\tvalid_1's l2: 0.479113\n",
      "CV score: 0.46930531\n"
     ]
    }
   ],
   "source": [
    "##### lgb_49\n",
    "lgb_49_param = {\n",
    "'num_leaves': 9,\n",
    "'min_data_in_leaf': 23,\n",
    "'objective':'regression',\n",
    "'max_depth': -1,\n",
    "'learning_rate': 0.002,\n",
    "\"boosting\": \"gbdt\",\n",
    "\"feature_fraction\": 0.45, \n",
    "\"bagging_freq\": 1,\n",
    "\"bagging_fraction\": 0.65, \n",
    "\"bagging_seed\": 15,\n",
    "\"metric\": 'mse',\n",
    "\"lambda_l2\": 0.2, \n",
    "\"verbosity\": -1} # 一个叶子上数据的最小数量 \\ feature_fraction将会在每棵树训练之前选择 45% 的特征。可以用来加速训练，可以用来处理过拟合。 #bagging_fraction不进行重采样的情况下随机选择部分数据。可以用来加速训练，可以用来处理过拟合。\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=9)   \n",
    "oof_lgb_49 = np.zeros(len(X_train_49))\n",
    "predictions_lgb_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train_49[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train_49[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 12000\n",
    "    lgb_49 = lgb.train(lgb_49_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n",
    "    oof_lgb_49[val_idx] = lgb_49.predict(X_train_49[val_idx], num_iteration=lgb_49.best_iteration)\n",
    "    predictions_lgb_49 += lgb_49.predict(X_test_49, num_iteration=lgb_49.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[23:48:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:48:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40422\tvalid_data-rmse:3.38325\n",
      "[500]\ttrain-rmse:0.52669\tvalid_data-rmse:0.71888\n",
      "[1000]\ttrain-rmse:0.43538\tvalid_data-rmse:0.72006\n",
      "[1201]\ttrain-rmse:0.40366\tvalid_data-rmse:0.72215\n",
      "fold n°2\n",
      "[23:48:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:48:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39826\tvalid_data-rmse:3.40774\n",
      "[500]\ttrain-rmse:0.52848\tvalid_data-rmse:0.70257\n",
      "[1000]\ttrain-rmse:0.43896\tvalid_data-rmse:0.70373\n",
      "[1200]\ttrain-rmse:0.40824\tvalid_data-rmse:0.70477\n",
      "fold n°3\n",
      "[23:48:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:48:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40193\tvalid_data-rmse:3.39292\n",
      "[500]\ttrain-rmse:0.53210\tvalid_data-rmse:0.66724\n",
      "[1000]\ttrain-rmse:0.43940\tvalid_data-rmse:0.66994\n",
      "[1056]\ttrain-rmse:0.43066\tvalid_data-rmse:0.67003\n",
      "fold n°4\n",
      "[23:48:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:48:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.40244\tvalid_data-rmse:3.39011\n",
      "[500]\ttrain-rmse:0.53292\tvalid_data-rmse:0.68121\n",
      "[1000]\ttrain-rmse:0.44117\tvalid_data-rmse:0.68323\n",
      "[1068]\ttrain-rmse:0.43044\tvalid_data-rmse:0.68367\n",
      "fold n°5\n",
      "[23:48:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[23:48:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3.39341\tvalid_data-rmse:3.42628\n",
      "[500]\ttrain-rmse:0.53509\tvalid_data-rmse:0.66088\n",
      "[1000]\ttrain-rmse:0.44116\tvalid_data-rmse:0.66250\n",
      "[1133]\ttrain-rmse:0.42068\tvalid_data-rmse:0.66356\n",
      "CV score: 0.47024327\n"
     ]
    }
   ],
   "source": [
    "##### xgb_49\n",
    "xgb_49_params = {'eta': 0.02, \n",
    "              'max_depth': 5, \n",
    "              'min_child_weight':3,\n",
    "              'gamma':0,\n",
    "              'subsample': 0.7, \n",
    "              'colsample_bytree': 0.35, \n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_49 = np.zeros(len(X_train_49))\n",
    "predictions_xgb_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train_49[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train_49[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_49 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_49_params)\n",
    "    oof_xgb_49[val_idx] = xgb_49.predict(xgb.DMatrix(X_train_49[val_idx]), ntree_limit=xgb_49.best_ntree_limit)\n",
    "    predictions_xgb_49 += xgb_49.predict(xgb.DMatrix(X_test_49), ntree_limit=xgb_49.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GradientBoostingRegressor梯度提升决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6569           0.0035           11.96s\n",
      "         2           0.6822           0.0032           11.95s\n",
      "         3           0.6477           0.0031           12.13s\n",
      "         4           0.6430           0.0032           12.07s\n",
      "         5           0.6543           0.0029           12.02s\n",
      "         6           0.6607           0.0033           12.08s\n",
      "         7           0.6260           0.0030           12.03s\n",
      "         8           0.6242           0.0030           11.91s\n",
      "         9           0.6273           0.0027           11.95s\n",
      "        10           0.6247           0.0028           11.98s\n",
      "        20           0.5923           0.0021           11.56s\n",
      "        30           0.5886           0.0021           11.28s\n",
      "        40           0.5607           0.0016           11.03s\n",
      "        50           0.5450           0.0013           10.82s\n",
      "        60           0.5135           0.0013           10.63s\n",
      "        70           0.4921           0.0009           10.45s\n",
      "        80           0.4807           0.0005           10.27s\n",
      "        90           0.4800           0.0007           10.11s\n",
      "       100           0.4625           0.0004            9.91s\n",
      "       200           0.4017           0.0001            7.93s\n",
      "       300           0.3693          -0.0000            5.91s\n",
      "       400           0.3458           0.0000            3.92s\n",
      "       500           0.3400          -0.0000            1.96s\n",
      "       600           0.3103          -0.0000            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6706           0.0033           12.29s\n",
      "         2           0.6535           0.0034           12.11s\n",
      "         3           0.6478           0.0035           12.05s\n",
      "         4           0.6428           0.0033           11.85s\n",
      "         5           0.6496           0.0032           11.74s\n",
      "         6           0.6479           0.0030           11.75s\n",
      "         7           0.6493           0.0031           12.51s\n",
      "         8           0.6277           0.0029           12.50s\n",
      "         9           0.6294           0.0028           12.38s\n",
      "        10           0.6427           0.0027           12.27s\n",
      "        20           0.6147           0.0026           11.65s\n",
      "        30           0.5861           0.0019           11.37s\n",
      "        40           0.5552           0.0018           11.15s\n",
      "        50           0.5293           0.0012           10.86s\n",
      "        60           0.5128           0.0010           10.63s\n",
      "        70           0.5042           0.0009           10.41s\n",
      "        80           0.4767           0.0006           10.23s\n",
      "        90           0.4664           0.0008           10.03s\n",
      "       100           0.4596           0.0006            9.83s\n",
      "       200           0.4113           0.0000            7.86s\n",
      "       300           0.3723          -0.0000            5.90s\n",
      "       400           0.3458          -0.0000            3.93s\n",
      "       500           0.3296          -0.0000            1.96s\n",
      "       600           0.3125          -0.0000            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6533           0.0037           11.99s\n",
      "         2           0.6744           0.0032           11.86s\n",
      "         3           0.6596           0.0029           12.26s\n",
      "         4           0.6396           0.0032           12.31s\n",
      "         5           0.6434           0.0034           12.26s\n",
      "         6           0.6314           0.0029           12.24s\n",
      "         7           0.6180           0.0030           12.25s\n",
      "         8           0.6345           0.0032           12.18s\n",
      "         9           0.6324           0.0031           12.12s\n",
      "        10           0.6294           0.0028           12.02s\n",
      "        20           0.5954           0.0023           11.54s\n",
      "        30           0.5829           0.0020           11.29s\n",
      "        40           0.5255           0.0019           11.09s\n",
      "        50           0.5214           0.0014           10.85s\n",
      "        60           0.5057           0.0012           10.64s\n",
      "        70           0.4849           0.0011           10.42s\n",
      "        80           0.4745           0.0008           10.24s\n",
      "        90           0.4771           0.0007           10.06s\n",
      "       100           0.4604           0.0006            9.85s\n",
      "       200           0.3910           0.0001            7.81s\n",
      "       300           0.3522          -0.0000            5.87s\n",
      "       400           0.3388           0.0000            3.91s\n",
      "       500           0.3139          -0.0000            1.96s\n",
      "       600           0.3029          -0.0000            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6721           0.0034           11.99s\n",
      "         2           0.6645           0.0034           11.67s\n",
      "         3           0.6684           0.0031           11.74s\n",
      "         4           0.6377           0.0035           11.62s\n",
      "         5           0.6721           0.0031           11.78s\n",
      "         6           0.6528           0.0031           11.78s\n",
      "         7           0.6484           0.0026           11.86s\n",
      "         8           0.6315           0.0031           11.84s\n",
      "         9           0.6378           0.0030           11.82s\n",
      "        10           0.6452           0.0029           11.73s\n",
      "        20           0.5769           0.0023           11.40s\n",
      "        30           0.5711           0.0022           11.16s\n",
      "        40           0.5370           0.0016           10.93s\n",
      "        50           0.5302           0.0013           10.69s\n",
      "        60           0.4979           0.0011           10.47s\n",
      "        70           0.4886           0.0008           10.27s\n",
      "        80           0.4778           0.0008           10.07s\n",
      "        90           0.4790           0.0006            9.89s\n",
      "       100           0.4615           0.0006            9.71s\n",
      "       200           0.3984           0.0001            7.73s\n",
      "       300           0.3702          -0.0001            5.81s\n",
      "       400           0.3473          -0.0000            3.89s\n",
      "       500           0.3370           0.0000            1.95s\n",
      "       600           0.3168          -0.0001            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6804           0.0030           12.33s\n",
      "         2           0.6545           0.0035           12.13s\n",
      "         3           0.6274           0.0034           12.06s\n",
      "         4           0.6387           0.0034           12.01s\n",
      "         5           0.6446           0.0031           11.97s\n",
      "         6           0.6287           0.0033           12.05s\n",
      "         7           0.6461           0.0031           12.06s\n",
      "         8           0.6462           0.0028           12.06s\n",
      "         9           0.6233           0.0030           12.15s\n",
      "        10           0.6396           0.0025           12.15s\n",
      "        20           0.5941           0.0023           11.61s\n",
      "        30           0.5645           0.0021           11.31s\n",
      "        40           0.5468           0.0016           11.05s\n",
      "        50           0.5348           0.0012           10.84s\n",
      "        60           0.5159           0.0012           10.65s\n",
      "        70           0.4880           0.0008           10.41s\n",
      "        80           0.4911           0.0007           10.21s\n",
      "        90           0.4713           0.0006            9.99s\n",
      "       100           0.4721           0.0005            9.81s\n",
      "       200           0.4020           0.0000            7.85s\n",
      "       300           0.3646           0.0000            5.86s\n",
      "       400           0.3474          -0.0000            3.90s\n",
      "       500           0.3303          -0.0001            1.95s\n",
      "       600           0.3118          -0.0000            0.00s\n",
      "CV score: 0.47090971\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr_49 = np.zeros(train_shape)\n",
    "predictions_gbr_49 = np.zeros(len(X_test_49))\n",
    "#GradientBoostingRegressor梯度提升决策树\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    gbr_49 = gbr(n_estimators=600, learning_rate=0.01,subsample=0.65,max_depth=6, min_samples_leaf=20,\n",
    "            max_features=0.35,verbose=1)\n",
    "    gbr_49.fit(tr_x,tr_y)\n",
    "    oof_gbr_49[val_idx] = gbr_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_gbr_49 += gbr_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上3种模型的基于49个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（Kernel Ridge Regression，核脊回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4673556519798529"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack3 = np.vstack([oof_lgb_49,oof_xgb_49,oof_gbr_49]).transpose()\n",
    "test_stack3 = np.vstack([predictions_lgb_49, predictions_xgb_49,predictions_gbr_49]).transpose()\n",
    "#\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack3 = np.zeros(train_stack3.shape[0])\n",
    "predictions_lr3 = np.zeros(test_stack3.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack3,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack3[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack3[val_idx], target.iloc[val_idx].values\n",
    "        #Kernel Ridge Regression\n",
    "    lr3 = kr()\n",
    "    lr3.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack3[val_idx] = lr3.predict(val_data)\n",
    "    predictions_lr3 += lr3.predict(test_stack3) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**383维的数据进行与上述263以及49维数据相同的操作**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kernel Ridge Regression 基于核的岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.54009041\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_kr_383 = np.zeros(train_shape)\n",
    "predictions_kr_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #Kernel Ridge Regression 岭回归\n",
    "    kr_383 = kr()\n",
    "    kr_383.fit(tr_x,tr_y)\n",
    "    oof_kr_383[val_idx] = kr_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_kr_383 += kr_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用普通岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.48687670\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_ridge_383 = np.zeros(train_shape)\n",
    "predictions_ridge_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #使用岭回归\n",
    "    ridge_383 = Ridge(alpha=1200)\n",
    "    ridge_383.fit(tr_x,tr_y)\n",
    "    oof_ridge_383[val_idx] = ridge_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_ridge_383 += ridge_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 使用ElasticNet 弹性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.53296555\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_en_383 = np.zeros(train_shape)\n",
    "predictions_en_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #ElasticNet 弹性网络\n",
    "    en_383 = en(alpha=1.0,l1_ratio=0.06)\n",
    "    en_383.fit(tr_x,tr_y)\n",
    "    oof_en_383[val_idx] = en_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_en_383 += en_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 使用BayesianRidge 贝叶斯岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.48717340\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_br_383 = np.zeros(train_shape)\n",
    "predictions_br_383 = np.zeros(len(X_test_383))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_383, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_383[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    #BayesianRidge 贝叶斯回归\n",
    "    br_383 = br()\n",
    "    br_383.fit(tr_x,tr_y)\n",
    "    oof_br_383[val_idx] = br_383.predict(X_train_383[val_idx])\n",
    "    \n",
    "    predictions_br_383 += br_383.predict(X_test_383) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_383, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们得到了以上4种模型的基于383个特征的预测结果以及模型架构及参数。其中在每一种特征工程中，进行5折的交叉验证，并重复两次（LinearRegression简单的线性回归），取得每一个特征数下的模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4883460396010735"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack1 = np.vstack([oof_br_383,oof_kr_383,oof_en_383,oof_ridge_383]).transpose()\n",
    "test_stack1 = np.vstack([predictions_br_383, predictions_kr_383,predictions_en_383,predictions_ridge_383]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack1 = np.zeros(train_stack1.shape[0])\n",
    "predictions_lr1 = np.zeros(test_stack1.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack1,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack1[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack1[val_idx], target.iloc[val_idx].values\n",
    "    # LinearRegression简单的线性回归\n",
    "    lr1 = lr()\n",
    "    lr1.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack1[val_idx] = lr1.predict(val_data)\n",
    "    predictions_lr1 += lr1.predict(test_stack1) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用更多的模型再次训练49维数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. KernelRidge 核岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.50254991\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_kr_49 = np.zeros(train_shape)\n",
    "predictions_kr_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    kr_49 = kr()\n",
    "    kr_49.fit(tr_x,tr_y)\n",
    "    oof_kr_49[val_idx] = kr_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_kr_49 += kr_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_kr_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ridge 岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.49451286\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_ridge_49 = np.zeros(train_shape)\n",
    "predictions_ridge_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    ridge_49 = Ridge(alpha=6)\n",
    "    ridge_49.fit(tr_x,tr_y)\n",
    "    oof_ridge_49[val_idx] = ridge_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_ridge_49 += ridge_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_ridge_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BayesianRidge 贝叶斯岭回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.49534595\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_br_49 = np.zeros(train_shape)\n",
    "predictions_br_49 = np.zeros(len(X_test_49))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    br_49 = br()\n",
    "    br_49.fit(tr_x,tr_y)\n",
    "    oof_br_49[val_idx] = br_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_br_49 += br_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_br_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ElasticNet 弹性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "fold n°2\n",
      "fold n°3\n",
      "fold n°4\n",
      "fold n°5\n",
      "CV score: 0.53841695\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_en_49 = np.zeros(train_shape)\n",
    "predictions_en_49 = np.zeros(len(X_test_49))\n",
    "#\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_49, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train_49[trn_idx]\n",
    "    tr_y = y_train[trn_idx]\n",
    "    en_49 = en(alpha=1.0,l1_ratio=0.05)\n",
    "    en_49.fit(tr_x,tr_y)\n",
    "    oof_en_49[val_idx] = en_49.predict(X_train_49[val_idx])\n",
    "    \n",
    "    predictions_en_49 += en_49.predict(X_test_49) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_en_49, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking 训练第二层模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4949139981666125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack4 = np.vstack([oof_br_49,oof_kr_49,oof_en_49,oof_ridge_49]).transpose()\n",
    "test_stack4 = np.vstack([predictions_br_49, predictions_kr_49,predictions_en_49,predictions_ridge_49]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack4 = np.zeros(train_stack4.shape[0])\n",
    "predictions_lr4 = np.zeros(test_stack4.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack4,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack4[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack4[val_idx], target.iloc[val_idx].values\n",
    "    #LinearRegression\n",
    "    lr4 = lr()\n",
    "    lr4.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack4[val_idx] = lr4.predict(val_data)\n",
    "    predictions_lr4 += lr4.predict(test_stack1) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 继续Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仅作参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4531767144415424"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#和下面作对比\n",
    "mean_squared_error(target.values, 0.7*(0.6*oof_stack2 + 0.4*oof_stack3)+0.3*(0.55*oof_stack1+0.45*oof_stack4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 LinearRegression 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "fold 6\n",
      "fold 7\n",
      "fold 8\n",
      "fold 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4484289295558744"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack5 = np.vstack([oof_stack1,oof_stack2,oof_stack3,oof_stack4]).transpose()\n",
    "test_stack5 = np.vstack([predictions_lr1, predictions_lr2,predictions_lr3,predictions_lr4]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack5 = np.zeros(train_stack5.shape[0])\n",
    "predictions_lr5= np.zeros(test_stack5.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack5,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack5[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack5[val_idx], target.iloc[val_idx].values\n",
    "    #LinearRegression\n",
    "    lr5 = lr()\n",
    "    lr5.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack5[val_idx] = lr5.predict(val_data)\n",
    "    predictions_lr5 += lr5.predict(test_stack5) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.82258159, 2.88475464, 3.47119513, ..., 4.00522306, 4.00204652,\n",
       "       4.90848617])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lr5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2968.000000\n",
       "mean        3.880151\n",
       "std         0.462130\n",
       "min         1.683074\n",
       "25%         3.669273\n",
       "50%         3.950815\n",
       "75%         4.186519\n",
       "max         5.047991\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_example = pd.read_csv('submit_example.csv',sep=',',encoding='latin-1')\n",
    "\n",
    "submit_example['happiness'] = predictions_lr5\n",
    "\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果数据进行清理(连续值->整数值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2968.000000\n",
       "mean        3.880139\n",
       "std         0.462061\n",
       "min         1.683074\n",
       "25%         3.669273\n",
       "50%         3.950815\n",
       "75%         4.186519\n",
       "max         5.000000\n",
       "Name: happiness, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_example.loc[submit_example['happiness']>4.96,'happiness']= 5\n",
    "submit_example.loc[submit_example['happiness']<=1.04,'happiness']= 1\n",
    "submit_example.loc[(submit_example['happiness']>1.96)&(submit_example['happiness']<2.04),'happiness']= 2\n",
    "\n",
    "submit_example.to_csv(\"submision.csv\",index=False)\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用网格搜索调Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数很多，需要继续研究"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
